---
title: "GVAR - Model Fitting and Results"
output:
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    smooth_scroll: yes
---

```{r setup, include = FALSE}
# Libraries
packages <- c(
  "tidyverse",
  "devtools",
  "readxl",
  "lubridate",
  "rmarkdown",
  "psych",
  "cmdstanr",
  "rstan",
  "loo",
  "bayesplot",
  "posterior",
  "bayestestR",
  "here",
  "rtf",
  "sjlabelled",
  "tsnet",
  "BGGM",
  "graphicalVAR",
  "mvtnorm"
)
#remotes::install_github("donaldRwilliams/BGGM")
#devtools::install_github("bsiepe/tsnet")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(packages, update = F, character.only = T)

source(here("scripts","functions.R"))
set.seed(35037)
```

### Simulate data
```{r}
gvar_mod <- graphicalVAR::randomGVARmodel(Nvar = 8,
                              probKappaEdge = .3,
                              probBetaEdge = .2)
# DGP matrices
gvar_mod$beta     # regression weights
gvar_mod$kappa    # precision matrix

# Simulate data from it
gvar_dat <- graphicalVAR::graphicalVARsim(nTime = 200, 
                                          beta = gvar_mod$beta,
                                          kappa = gvar_mod$kappa)

# Additionally, use data from our preprint
l_graphs <- readRDS(here("data", "l_graphs.rds"))

# Choose graph 6
gvar_mod <- l_graphs$graph6

# Add partial correlations
gvar_mod$PCC <- -cov2cor(gvar_mod$kappa)
diag(gvar_mod$PCC) <- 0

# Simulate data
gvar_dat <- graphicalVAR::graphicalVARsim(nTime = 200, 
                                          beta = gvar_mod$beta,
                                          kappa = gvar_mod$kappa)


```

Scale data
```{r}
Y <- gvar_dat %>% apply(., 2, scale)
K <- ncol(gvar_dat)
n_t <- nrow(gvar_dat)
```

### Specify Priors
```{r}
# prior on the locations of partial correlations (scaled Beta location)
prior_Rho_loc <- matrix(.5, nrow = K, ncol = K)
# prior_Rho_scale <- matrix(sqrt(.5), nrow = K, ncol = K) # sqrt(mu) corresponds to uniform distribution
prior_Rho_scale <- matrix(.4, nrow = K, ncol = K) # bit more regularization

prior_Beta_loc <- matrix(0, nrow = K, ncol = K)
prior_Beta_scale <- matrix(.5, nrow = K, ncol = K)  # regularize a bit
```

#### Plot prior choices for scaled beta
BS: Would be good to not make this uniform, high partial correlations are quite unrealistic.
Would decrease the prior sd for the partial correlations
```{r}
mu <- .5
sd <- .4
a <- mu / (sd * sd)
b <- (1-mu) / (sd * sd)
hist((rbeta(1e6,a, b) - .5) * 2)
```

## Fit VAR Model in Stan
```{r stan data & compilation}
# prepare stan data
stan_data <-
  list(K = K,
       "T" = n_t,
       Y = as.matrix(Y),
       prior_Rho_loc = prior_Rho_loc,
       prior_Rho_scale = prior_Rho_scale,
       prior_Beta_loc = prior_Beta_loc,
       prior_Beta_scale = prior_Beta_scale
       )
# Choose model to fit
model_name <- "VAR_lkj"
# number of MCMC chains
n_chains <- 4
```

```{r sampling, message=FALSE, eval=FALSE}
# Compile model
var_lkj_model <- cmdstanr::cmdstan_model(
  stan_file = here("scripts", paste0(model_name, ".stan")),
  pedantic = TRUE#,   
  #quiet = FALSE
  )
# Run sampler
var_lkj_fit <- var_lkj_model$sample(
  data = stan_data,
  seed = 2023,
  chains = n_chains,
  parallel_chains = n_chains,
  iter_warmup = 500,
  iter_sampling = 500,
  refresh = 500,
  thin = 1,
  adapt_delta = .8,
  init = .1
)
# time to fit
var_lkj_fit$time()$total
```

```{r}
# Compile model
var_lkj_model <- rstan::stan_model(
  file = here("scripts", paste0(model_name, ".stan")))
# Run sampler
var_lkj_fit <- rstan::sampling(
  object = var_lkj_model,
  data = stan_data,
  seed = 2023,
  chains = n_chains,
  cores = n_chains,
  iter = 1000,
  warmup = 500,
  refresh = 500,
  thin = 1,
  init = .1,
  control = list(adapt_delta = .8)
)
```

## Effective sample size (ESS) & Rhat Plots
```{r}
# color scheme
color_scheme_set(scheme = "purple")
# Effective sample sizes
plot_neff <-
  mcmc_neff_hist(bayesplot::neff_ratio(var_lkj_fit), binwidth = .01) +
  labs(title = "A") +
  guides(color = "none", fill = "none") +
  theme(
    legend.text = element_blank(),
    legend.key = element_blank(),
    title = element_text(size = 16, face = "bold")
  )
# Rhat
# BS: Why does this have missings?
plot_rhat <-
  bayesplot::mcmc_rhat_hist(bayesplot::rhat(var_lkj_fit)) +
  labs(title = "B") +
  guides(color = "none", fill = "none") +
  theme(
    legend.text = element_blank(),
    legend.key = element_blank(),
    title = element_text(size = 16, face = "bold")
  ) +
  yaxis_text(on = TRUE)
# Combined plot
plot_diagnostics <- gridExtra::grid.arrange(plot_neff, plot_rhat, ncol = 2)
```

### LOO
```{r}
var_lkj_loo <- var_lkj_fit$loo()
print(var_lkj_loo)
plot(var_lkj_loo)
```

```{r}
#param_ests <- var_lkj_fit$summary(c("B", "Rho","mu_B","sigma_B", "sigma_theta"))
param_ests <- var_lkj_fit$summary()
```

# Parameter Recovery

### Betas
```{r}
medians_beta <-
  var_lkj_fit$summary("Beta") %>% 
  dplyr::select(median) %>% 
  unlist() %>% 
  round(2) %>% 
  matrix(., nrow = K, byrow = FALSE)
cor.plot(medians_beta)

cor.plot(gvar_mod$beta)
```

### Difference in Betas
```{r}
diff_Rho <- medians_beta - gvar_mod$beta
cor.plot(diff_Rho)
```

```{r}
draws_beta <- var_lkj_fit$draws("Beta") %>% as_draws_matrix()
bayesplot::ppc_intervals(gvar_mod$beta %>% as.vector(),draws_beta)
```

### Partial Correlations
```{r}
medians_rho <-
  var_lkj_fit$summary("Rho") %>% 
  dplyr::select(median) %>% 
  unlist() %>% 
  round(2) %>% 
  matrix(., nrow = K, byrow = FALSE)
cor.plot(medians_rho)

cor.plot(gvar_mod$PCC)

```

### Difference in partial correlations
```{r}
diff_Rho <- medians_rho - gvar_mod$PCC
cor.plot(diff_Rho)
```


```{r}
draws_rho <- var_lkj_fit$draws("Rho") %>% as_draws_matrix()
bayesplot::ppc_intervals(gvar_mod$PCC %>% as.vector(),draws_rho)
```

***

# Loglik extraction
```{r}
source(here("scripts","functions.R"))
draws_sigma <- var_lkj_fit$draws("Sigma") %>% as_draws_matrix()

log_lik_0 <-
  log_lik_gVAR(Y = Y,
               draws_beta = draws_beta,
               draws_sigma = draws_sigma, 
               n_cores = 4)

chain_ids <- var_lkj_fit$draws("Beta") %>% 
  as_draws_df() %>% 
  dplyr::select(.chain) %>% unlist()

loo_0 <- loo(log_lik_0, r_eff = relative_eff(log_lik_0, chain_ids)
)

```


# Posterior comparisons

Compare data structures of BGGM and the stan model/posterior package
```{r}
# Load data of two individuals
data <- BGGM::ifit
data_1 <- subset(data, id == 1) 
data_3 <- subset(data, id == 3) 

# Estimate networks
# (should perform detrending etc. in a real use case)
net_1_bggm <- BGGM::var_estimate(data_1[,-1],
                            rho_sd = 0.25, 
                            Beta_sd = 0.5,
                            iter = 50000)
net_3_bggm <- BGGM::var_estimate(data_3[,-1],
                            rho_sd = 0.25, 
                            Beta_sd = 0.5,
                            iter = 50000)

```

### Specify Priors
```{r}
Y_1 <- data_1[,-1] %>% apply(., 2, scale)
K <- ncol(Y_1)
n_t <- nrow(Y_1)
prior_Rho_loc <- matrix(.5, nrow = K, ncol = K)
prior_Rho_scale <- matrix(.4, nrow = K, ncol = K)
prior_Beta_loc <- matrix(0, nrow = K, ncol = K)
prior_Beta_scale <- matrix(.5, nrow = K, ncol = K)  # regularize a bit

priors <- list(
  prior_Rho_loc = prior_Rho_loc,
  prior_Rho_scale = prior_Rho_scale,
  prior_Beta_loc =  prior_Beta_loc,
  prior_Beta_scale = prior_Beta_scale
)

net_1_stan <- fit_gVAR_stan(data = Y_1, priors = priors)


Y_3 <- data_3[,-1] %>% apply(., 2, scale)
K <- ncol(Y_3)
n_t <- nrow(Y_3)
prior_Rho_loc <- matrix(.5, nrow = K, ncol = K)
prior_Rho_scale <- matrix(.4, nrow = K, ncol = K)
prior_Beta_loc <- matrix(0, nrow = K, ncol = K)
prior_Beta_scale <- matrix(.5, nrow = K, ncol = K)  # regularize a bit

net_3_stan <- fit_gVAR_stan(data = Y_3, priors = priors)
```

Posterior samples are in a slightly different format when we use the BGGM package
in BGGM, they are in a tensor format. I think this will be the most efficient format for the test.

```{r}
net_1_bggm$fit$beta
draws_beta %>% View()
```

```{r}

draws_net_1_beta_bggm <-  lapply(1:dim(net_1_bggm$fit$beta)[3], function(n){
  net_1_bggm$fit$beta[,,n]
})

draws_net_3_beta_bggm <-  lapply(1:dim(net_3_bggm$fit$beta)[3], function(n){
  net_3_bggm$fit$beta[,,n]
})

beta_diff_post <- mat_diff_L2(draws_net_1_beta_bggm, draws_net_3_beta_bggm)
beta_diff_prior <- chi::rchi(n = length(beta_diff_post), df = ncol(beta_post_1[[1]]), ncp = 0)



data.frame(posterior = beta_diff_post, prior = beta_diff_prior) %>% 
  ggplot() +
  geom_histogram(aes(posterior), fill = "red", alpha = .5, binwidth = .1) +
  geom_histogram(aes(prior), fill = "blue", alpha = .5,  binwidth = .1) +
  xlim(0,NA)

bf <- bayestestR::bayesfactor_parameters(posterior = beta_diff_post, prior = beta_diff_prior)
bf
bf$log_BF
```


### Compare Models for same True Parameters

Simulate data from same model, slightly different DGP, strongly different DGP
```{r}
l_changed_graphs_0305 <- readRDS(here("data/l_changed_graphs_0305.rds"))

gvar_mod1 <- l_changed_graphs_0305[["graph5"]][["truegraph"]]
gvar_mod2 <- l_changed_graphs_0305[["graph5"]][["const0.05"]]
gvar_mod3 <- l_changed_graphs_0305[["graph5"]][["const0.15"]]

t <- 500

data_1a <- graphicalVAR::graphicalVARsim(nTime = t,
                                         beta = gvar_mod1$beta,
                                         kappa = gvar_mod1$kappa)

data_1b <- graphicalVAR::graphicalVARsim(nTime = t,
                                         beta = gvar_mod1$beta,
                                         kappa = gvar_mod1$kappa)


data_2 <- graphicalVAR::graphicalVARsim(nTime = t,
                                        beta = gvar_mod2$beta,
                                        kappa = gvar_mod2$kappa)

data_3 <- graphicalVAR::graphicalVARsim(nTime = t,
                                        beta = gvar_mod3$beta,
                                        kappa = gvar_mod3$kappa)

K <- ncol(data_1a)
prior_Rho_loc <- matrix(.5, nrow = K, ncol = K)
prior_Rho_scale <- matrix(.4, nrow = K, ncol = K)
prior_Beta_loc <- matrix(0, nrow = K, ncol = K)
prior_Beta_scale <- matrix(.5, nrow = K, ncol = K)

source(here("scripts", "functions.R"))
priors <- list(
  prior_Rho_loc = prior_Rho_loc,
  prior_Rho_scale = prior_Rho_scale,
  prior_Beta_loc =  prior_Beta_loc,
  prior_Beta_scale = prior_Beta_scale
)
```

Fit networks 
```{r message=FALSE}
net_1a_fit <-
  fit_gVAR_stan(
    data = data_1a,
    priors = NULL,
    iter_sampling = 500,
    method = "sampling"
  )
net_1b_fit <-
  fit_gVAR_stan(
    data = data_1b,
    priors = NULL,
    iter_sampling = 500,
    method = "sampling"
  )
net_2_fit <-
  fit_gVAR_stan(
    data = data_2,
    priors = NULL,
    iter_sampling = 500,
    method = "sampling"
  )
net_3_fit <-
  fit_gVAR_stan(
    data = data_3,
    priors = NULL,
    iter_sampling = 500,
    method = "sampling"
  )
```

```{r}
beta_post_1a <-
  rstan::extract(net_1a_fit, "Beta", permuted = FALSE) %>% as_draws_matrix() 
beta_post_1b <-
  rstan::extract(net_1b_fit, "Beta", permuted = FALSE) %>% as_draws_matrix() 
beta_post_2 <-
  rstan::extract(net_2_fit, "Beta", permuted = FALSE) %>% as_draws_matrix()
beta_post_3 <-
  rstan::extract(net_3_fit, "Beta", permuted = FALSE) %>% as_draws_matrix()

beta_post_1a[c(2,5),c(3,1)]
```

```{r}
compare_mat <- function(mat1, mat2, bootstrap_samples = 0, plot = TRUE) {
  
  # with bootstrapping
  if (bootstrap_samples > 0) {
  rows1 <- sample(1:nrow(mat1), size = bootstrap_samples*ncol(mat1), replace = TRUE)
  rows2<- sample(1:nrow(mat2), size = bootstrap_samples*ncol(mat1), replace = TRUE)
  cols <- rep.int(1:ncol(mat1), times = bootstrap_samples)

  col1 <- purrr::map2(.x = rows1, .y = cols, .f = function(.x,.y){mat1[.x, .y]}) %>% 
    unlist()
  col2 <- purrr::map2(.x = rows2, .y = cols, .f = function(.x,.y){mat2[.x, .y]}) %>% 
    unlist()
  
  diff_post <- (col1 - col2) ^ 2 %>%
    matrix(., nrow = bootstrap_samples, ncol = ncol(mat1), byrow = TRUE) %>%
    apply(., 1, sum) %>%
    sqrt()

  # no bootstrappping
  }else{
    diff_post <- mat1 - mat2
  attr(diff_post, which = "class") <- "matrix"
  diff_post <- diff_post %>% 
    apply(., 2, function(x) {
      x ^ 2
    }) %>%
    apply(., 1, sum) %>% sqrt()
  }
  
    diff_prior <-
    (rnorm(
      n = nrow(mat1) * ncol(mat1),
      mean = 0,
      sd = 0.5
    ) -
      rnorm(
        n = nrow(mat1) * ncol(mat1),
        mean = 0,
        sd = 0.5
      )) ^ 2 %>%
    matrix(., nrow = nrow(mat1), ncol = ncol(mat1)) %>%
    apply(., 1, sum) %>% 
    sqrt()
  
  diff_null <-
    (runif(
      n = nrow(mat1) * ncol(mat1),
      min = -.05 ,
      max = .05
    ) -
      runif(
        n = 1000,
        min = -.05 ,
        max = .05
      )) ^ 2 %>%
    matrix(., nrow = nrow(mat1), ncol = ncol(mat1)) %>%
    apply(., 1, sum) %>%
    sqrt()
    
    null_ci <- bayestestR::hdi(diff_null)
  
  # plot prior vs. postirior
  if (isTRUE(plot)) {
    plot <- data.frame(posterior = diff_post, prior = diff_prior) %>%
      ggplot2::ggplot() +
      ggplot2::geom_density(aes(posterior),
                            fill = "red",
                            alpha = .5) +
      ggplot2::geom_density(aes(prior),
                            fill = "blue",
                            alpha = .5) +
      # ggplot2::geom_histogram(aes(posterior),
      #                       fill = "red",
      #                       alpha = .5, 
      #                       binwidth = .01) +
      # ggplot2::geom_histogram(aes(prior),
      #                       fill = "blue",
      #                       alpha = .5,
      #                       binwidth = .01) +
      ggplot2::xlim(0, 3) +
      ggplot2::ylim(0, NA)
    
    print(plot)
  }
  
  # compute bayesfactor
  bf <-
    bayestestR::bf_parameters(
      posterior = diff_post,
      prior = diff_prior,
      null = c(0, null_ci$CI_high),
      direction = ">"
    )
  print(paste("BF:", exp(bf$log_BF)))
  print(paste("log-BF:", round(bf$log_BF, 2)))
  
  return(bf$log_BF)
  
  # clean up
  rm(col1, col2, diff_post, diff_prior, diff_null)
}
```

```{r}
compare_mat(
  mat1 = beta_post_1a,
  mat2 = beta_post_1b,
  bootstrap_samples = 1e4,
  plot = TRUE
)
compare_mat(
  mat1 = beta_post_1a,
  mat2 = beta_post_2,
  bootstrap_samples = 1e4,
  plot = FALSE
)
compare_mat(
  mat1 = beta_post_1a,
  mat2 = beta_post_3,
  bootstrap_samples = 1e4,
  plot = FALSE
) 
```

```{r}
log_BFs_boot <- rep(NA, 20)
for (i in 1:20) {
  log_BFs_boot[i] <- compare_mat(
    mat1 = beta_post_1a,
    mat2 = beta_post_1b,
    bootstrap_samples = 1e4,
    plot = FALSE
  )
}

log_BFs <- rep(NA, 20)
for (i in 1:20) {
  log_BFs[i] <- compare_mat(
    mat1 = beta_post_1a,
    mat2 = beta_post_1b,
    bootstrap_samples = 1e4,
    plot = FALSE
  )
}

            
```


```{r}
# mat_diff_L2 <- function(mat_post_1, mat_post_2) {
#   diff <- lapply(1:length(mat_post_1), function(n) {
#     sum((mat_post_1[[n]] - mat_post_2[[n]]) ^ 2)
#   })
#   diff <- unlist(diff)
#   return(diff)
# } 
```









```{r stan data & compilation}
beta_post_1 <- rstan::extract(net_1a_fit, "Beta", permuted = FALSE) %>% as_draws_matrix()
beta_post_2 <- rstan::extract(net_2_fit, "Beta", permuted = FALSE) %>% as_draws_matrix()

# prepare stan data
diff_test_stan_data <-
  list(P = ncol(beta_post_1),
       N = nrow(beta_post_1),
       Y_1 = beta_post_1,
       Y_2 = beta_post_2
       )
# Choose model to fit
model_name <- "chisqr_diff_test"
# number of MCMC chains
n_chains <- 4
```


This now "runs", but the chains don't really mix and everything is broken. 
```{r}
# Compile model
diff_test_model <- rstan::stan_model(
  file = here("scripts", paste0(model_name, ".stan")))
# Run sampler
diff_test_fit <- rstan::sampling(
  object = diff_test_model,
  data = diff_test_stan_data,
  seed = 2023,
  chains = n_chains,
  cores = n_chains,
  iter = 1000,
  warmup = 500,
  refresh = 500,
  thin = 1,
  init = .1,
  control = list(adapt_delta = .8)
)
```


```{r}
source(here("scripts","functions.R"))

cores <- parallel::detectCores() - 4
net_1_data_1_loo <- loo_gVAR(stan_fit = net_1_fit, data = data_1, n_cores = cores)
net_1_data_2_loo <- loo_gVAR(stan_fit = net_1_fit, data = data_2, n_cores = cores)
net_2_data_2_loo <- loo_gVAR(stan_fit = net_2_fit, data = data_2, n_cores = cores)
net_2_data_1_loo <- loo_gVAR(stan_fit = net_2_fit, data = data_1, n_cores = cores)
```

```{r}
plot(net_1_data_1_loo)
plot(net_1_data_2_loo)
plot(net_2_data_2_loo)
plot(net_2_data_1_loo)
```

```{r}
loo_compare(net_1_data_1_loo, net_1_data_2_loo)
loo_compare(net_2_data_2_loo, net_2_data_1_loo)
```


```{r}
net_1_data_1_logml <- bridgesampling::bridge_sampler(
  samples = net_1_fit,
  stan_fit = net_1_fit
)$logml

net_1_data_2_logml <- bridgesampling::bridge_sampler(
  samples = net_1_fit,
  stan_fit = net_1_fit,
  log_posterior = log_lik_gVAR,
  data = data_2
)$logml

net_2_data_2_logml <- bridgesampling::bridge_sampler(
  samples = net_2_fit,
  stan_fit = net_2_fit
)$logml

net_2_data_1_logml <- bridgesampling::bridge_sampler(
  samples = net_2_fit,
  stan_fit = net_2_fit,
  log_posterior = log_lik_gVAR,
  data = data_1
)$logml

```

```{r}
bridgesampling::bayes_factor(net_1_data_1_logml, net_1_data_2_logml)
bridgesampling::bayes_factor(net_2_data_2_logml, net_2_data_1_logml)
```


```{r}
(net_1_data_1_loo$elpd - net_1_data_1_logml) - (net_1_data_2_loo$elpd - net_1_data_2_logml) 
```


```{r}
var_lkj_draws <- rstan::extract(var_lkj_fit)
var_lkj_LML <- bridgesampling::bridge_sampler(
  samples = var_lkj_fit,
  stan_fit = var_lkj_fit,
  method = "warp3"
)
bridgesampling::logml(var_lkj_LML)
var_lkj_LML$logml
```




# Compare Variational Inference
```{r}
source(here("scripts","functions.R"))
Y <- gvar_dat %>% apply(., 2, scale)
# Run variational
var_lkj_fit_vb <- fit_gVAR_stan(data = Y,backend = "rstan", method = "variational")
# time to fit
var_lkj_fit_vb$time()$total
```

```{r}
medians_Beta_vb <-
  summary(var_lkj_fit_vb, "Beta") %>% 
  dplyr::select(median) %>% 
  unlist() %>% 
  round(2)
plot(medians_Beta_vb, medians_beta %>% as.vector())
abline(coef = c(0,1))
```


```{r}
medians_rho_vb <-
  var_lkj_fit_vb$summary("Rho") %>% 
  dplyr::select(median) %>% 
  unlist() %>% 
  round(2)

# This looks abit more ugly now!
plot(medians_rho_vb, medians_rho %>% as.vector())
abline(coef = c(0,1))
```

```{r}
draws_Beta_vb <- var_lkj_fit_vb$draws("Beta") %>% as_draws_matrix()
bayesplot::ppc_intervals(gvar_mod$beta %>% as.vector(),draws_beta)+
    ylim(c(-1,1))
bayesplot::ppc_intervals(gvar_mod$beta %>% as.vector(),draws_Beta_vb)+
    ylim(c(-1,1))
```

```{r}
draws_rho_vb <- var_lkj_fit_vb$draws("Rho") %>% as_draws_matrix()
bayesplot::ppc_intervals(gvar_mod$PCC %>% as.vector(),draws_rho)+
  ylim(c(-.7,.7))
bayesplot::ppc_intervals(gvar_mod$PCC %>% as.vector(),draws_rho_vb)+
    ylim(c(-.7,.7))
```

***



